{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff19cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7863, 5)\n",
      "(7840, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "2.0    0.855740\n",
       "0.0    0.103061\n",
       "1.0    0.041199\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "from numpy.random import randint\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, ConfusionMatrixDisplay, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load your DataFrame with columns 'comments' and 'sentiments'\n",
    "# Replace 'your_dataframe.csv' with the actual file path\n",
    "df = pd.read_csv(\"/Users/ethanshen/Documents/UIUC/Fa23/CS410/GroupProject/reviews.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "# removing null comments\n",
    "df = df[~df['Comment'].isnull()].reset_index()\n",
    "print(df.shape)\n",
    "\n",
    "# setting target\n",
    "df['Sentiment'] = np.where(\n",
    "    df['Rating'].isin([1,2]), 0,\n",
    "    np.where(\n",
    "        df['Rating'].isin([4,5]), 2,\n",
    "        np.where(\n",
    "            df['Rating'].isin([3]), 1, np.nan\n",
    "        )\n",
    "    )\n",
    ")\n",
    "df['Sentiment'].value_counts(True,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da911ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/prakharrathi25/sentiment-analysis-using-bert\n",
    "# https://datascience.stackexchange.com/questions/116103/do-sampling-before-or-after-tfidf-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78b71564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ethanshen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0               comfortable take light used personal work\n",
       "1                               lightweight great machine\n",
       "2             writing review macbook air two years owning\n",
       "3       little macbooks ideal students small mature bu...\n",
       "4       love laptop currently typing sometimes screen ...\n",
       "                              ...                        \n",
       "7835    bought sandisk class use htc getting constant ...\n",
       "7836    used extending capabilities samsung galaxy not...\n",
       "7837    great card fast comes optional adapter sd clas...\n",
       "7838                    good amount space stuff want fits\n",
       "7839    heard bad things micro sd card crapping weeks ...\n",
       "Name: Comment_v2, Length: 7840, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords and punctuation to comments\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def preprocess_text(text):\n",
    "    words = [w.lower() for w in text.split() if (w.isalpha() and w.lower() not in stopwords)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['Comment_v2'] = df['Comment'].apply(preprocess_text)\n",
    "df['Comment_v2'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890d6ce",
   "metadata": {},
   "source": [
    "# Modeling with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4210a28",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3dd58",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf06e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_vectorizer = TfidfVectorizer(strip_accents=None,\n",
    "                             lowercase=False,\n",
    "                             preprocessor=None,\n",
    "                             ngram_range = (1,1))\n",
    "\n",
    "\n",
    "X_uni = uni_vectorizer.fit_transform(df[\"Comment_v2\"].values.astype('U'))\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "X_uni_train, X_uni_test, y_train, y_test = train_test_split(X_uni, y, train_size=0.70, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2fa0362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8673469387755102\n",
      "0.9120738098094888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  48,    0,  209],\n",
       "       [   3,    0,   91],\n",
       "       [   9,    0, 1992]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_uni = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state = 42).fit(X_uni_train, y_train)\n",
    "logreg_uni_preds = logreg_uni.predict(X_uni_test)\n",
    "print(accuracy_score(logreg_uni_preds,y_test))\n",
    "print(f1_score(logreg_uni_preds, y_test, average='weighted'))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, logreg_uni_preds)\n",
    "cm\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209749f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ff3ba",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caf01305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuning_path = r'/Users/ethanshen/Documents/UIUC/Fa23/CS410/GroupProject/unigram_xgboost.csv'\n",
    "# if os.path.exists(tuning_path):\n",
    "#     pass\n",
    "# else:\n",
    "#     print(\"Now we're tuning hyperparameters for {} iterations ~~>=__=<~~\".format(300))\n",
    "#     header = 'lr,tree,leaf,depth,subsample,colsample,auc_train,auc_test'\n",
    "#     with open(tuning_path, 'w') as f:\n",
    "#         f.write(header)\n",
    "#         f.write('\\n')\n",
    "#     for i in range(300):\n",
    "#         lr = uniform(low=0.01, high=0.1)\n",
    "#         tree = randint(low=100, high=1000)\n",
    "#         leaf = randint(low=3, high=8)\n",
    "#         depth = randint(low=3, high=6)\n",
    "#         subsample = uniform(low=0.5, high=1)\n",
    "#         colsample = uniform(low=0.5, high=1)\n",
    "#         params = {'learning_rate': lr,\n",
    "#                 'n_estimators': tree,\n",
    "#                 'num_leaves': leaf,\n",
    "#                 'max_depth': depth,\n",
    "#                 'subsample': subsample,\n",
    "#                 'colsample_bytree': colsample,\n",
    "#                 'objective': 'multi:softmax',\n",
    "#                 'random_state': 42}\n",
    "#         xgb = XGBClassifier(**params)\n",
    "#         xgb.fit(X_uni_train, y_train)\n",
    "\n",
    "#         # train\n",
    "#         train_preds = xgb.predict(X_uni_train)\n",
    "#         train_f1score = f1_score(train_preds, y_train, average='weighted')\n",
    "        \n",
    "#         # test\n",
    "#         test_preds = xgb.predict(X_uni_test)\n",
    "#         test_f1score = f1_score(test_preds, y_test, average='weighted')\n",
    "\n",
    "#         line = ','.join(str(i) for i in [lr, tree, leaf, depth, subsample, colsample, \n",
    "#                                             train_f1score, test_f1score])\n",
    "#         # print(\"Line {} is written into result\".format(line))\n",
    "#         with open(tuning_path, 'a') as f:\n",
    "#             f.write(line)\n",
    "#             f.write('\\n')\n",
    "#         print(\"Hyperparameter round {} : {} is finished\".format(i, line))\n",
    "#     print(\"~~~~Hyperparameter tuning done~~~~\")\n",
    "\n",
    "# xgb_uni_tuning = pd.read_csv(tuning_path)\n",
    "# xgb_uni_tuning.sort_values('auc_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "896db986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/data.py:365: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8520408163265306\n",
      "0.9133181925552738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  10,    0,  247],\n",
       "       [   2,    0,   92],\n",
       "       [   7,    0, 1994]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_uni_params = {\n",
    "    'learning_rate': 0.014648,\n",
    "    'n_estimators': 103,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.582055,\n",
    "    'colsample_bytree': 0.521305,\n",
    "    'objective': 'multi:softmax',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_uni = XGBClassifier(**xgb_uni_params)\n",
    "xgb_uni.fit(X_uni_train, y_train)\n",
    "xgb_uni_preds = xgb_uni.predict(X_uni_test)\n",
    "print(accuracy_score(xgb_uni_preds,y_test))\n",
    "print(f1_score(xgb_uni_preds, y_test, average='weighted'))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, xgb_uni_preds)\n",
    "confusion_matrix\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6aac8",
   "metadata": {},
   "source": [
    "## Oversampling Neutral and Negative Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e3165",
   "metadata": {},
   "source": [
    "Option 1 with TFIDF before SMOTE: using the true IDF weights corresponding to the original text. Disadvantage: not consistent with the modified frequencies caused by SMOTE.\n",
    "\n",
    "https://datascience.stackexchange.com/questions/116103/do-sampling-before-or-after-tfidf-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95fc7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_uni_train_over, y_train_over = sm.fit_resample(X_uni_train, y_train)\n",
    "X_uni_test_over, y_test_over = X_uni_test.copy(), y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b62e5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos, Neu, Neg rates in train are 33.33% 33.33% 33.33%\n",
      "Pos, Neu, Neg rates in test are 85.08% 4.00% 10.93%\n"
     ]
    }
   ],
   "source": [
    "print(\"Pos, Neu, Neg rates in train are {:.2%} {:.2%} {:.2%}\".format((y_train_over == 2).sum() / y_train_over.shape[0], (y_train_over == 1).sum() / y_train_over.shape[0], (y_train_over == 0).sum() / y_train_over.shape[0]))\n",
    "print(\"Pos, Neu, Neg rates in test are {:.2%} {:.2%} {:.2%}\".format((y_test_over == 2).sum() / y_test_over.shape[0], (y_test_over == 1).sum() / y_test_over.shape[0], (y_test_over == 0).sum() / y_test_over.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9d7c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8303571428571429\n",
      "0.8200123312437336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 173,   21,   63],\n",
       "       [  35,   14,   45],\n",
       "       [ 171,   64, 1766]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_uni_over = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42).fit(X_uni_train_over, y_train_over)\n",
    "logreg_uni_preds_over = logreg_uni_over.predict(X_uni_test_over)\n",
    "print(accuracy_score(logreg_uni_preds_over,y_test_over))\n",
    "print(f1_score(logreg_uni_preds_over, y_test_over, average='weighted'))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_over, logreg_uni_preds_over)\n",
    "cm\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40358479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>tree</th>\n",
       "      <th>depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.091607</td>\n",
       "      <td>843</td>\n",
       "      <td>5</td>\n",
       "      <td>0.871868</td>\n",
       "      <td>0.827515</td>\n",
       "      <td>0.991559</td>\n",
       "      <td>0.877419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.093393</td>\n",
       "      <td>928</td>\n",
       "      <td>4</td>\n",
       "      <td>0.846220</td>\n",
       "      <td>0.539752</td>\n",
       "      <td>0.988578</td>\n",
       "      <td>0.875837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.056348</td>\n",
       "      <td>955</td>\n",
       "      <td>5</td>\n",
       "      <td>0.594291</td>\n",
       "      <td>0.849217</td>\n",
       "      <td>0.985241</td>\n",
       "      <td>0.875101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.083384</td>\n",
       "      <td>806</td>\n",
       "      <td>4</td>\n",
       "      <td>0.969091</td>\n",
       "      <td>0.643170</td>\n",
       "      <td>0.986584</td>\n",
       "      <td>0.874548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.067575</td>\n",
       "      <td>715</td>\n",
       "      <td>5</td>\n",
       "      <td>0.699880</td>\n",
       "      <td>0.985070</td>\n",
       "      <td>0.985024</td>\n",
       "      <td>0.874418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016509</td>\n",
       "      <td>175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.597772</td>\n",
       "      <td>0.744069</td>\n",
       "      <td>0.751950</td>\n",
       "      <td>0.795040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.016845</td>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>0.520360</td>\n",
       "      <td>0.852509</td>\n",
       "      <td>0.774751</td>\n",
       "      <td>0.791482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.019448</td>\n",
       "      <td>131</td>\n",
       "      <td>3</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.890950</td>\n",
       "      <td>0.739880</td>\n",
       "      <td>0.788354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.013627</td>\n",
       "      <td>208</td>\n",
       "      <td>3</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.963999</td>\n",
       "      <td>0.751916</td>\n",
       "      <td>0.787963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.016592</td>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>0.653969</td>\n",
       "      <td>0.796891</td>\n",
       "      <td>0.707786</td>\n",
       "      <td>0.780252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lr  tree  depth  subsample  colsample  auc_train  auc_test\n",
       "251  0.091607   843      5   0.871868   0.827515   0.991559  0.877419\n",
       "233  0.093393   928      4   0.846220   0.539752   0.988578  0.875837\n",
       "187  0.056348   955      5   0.594291   0.849217   0.985241  0.875101\n",
       "256  0.083384   806      4   0.969091   0.643170   0.986584  0.874548\n",
       "158  0.067575   715      5   0.699880   0.985070   0.985024  0.874418\n",
       "..        ...   ...    ...        ...        ...        ...       ...\n",
       "4    0.016509   175      3   0.597772   0.744069   0.751950  0.795040\n",
       "28   0.016845   141      4   0.520360   0.852509   0.774751  0.791482\n",
       "178  0.019448   131      3   0.676142   0.890950   0.739880  0.788354\n",
       "132  0.013627   208      3   0.515400   0.963999   0.751916  0.787963\n",
       "199  0.016592   115      3   0.653969   0.796891   0.707786  0.780252\n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_path = r'/Users/ethanshen/Documents/UIUC/Fa23/CS410/GroupProject/unigram_oversample_xgboost.csv'\n",
    "if os.path.exists(tuning_path):\n",
    "    pass\n",
    "else:\n",
    "    print(\"Now we're tuning hyperparameters for {} iterations ~~>=__=<~~\".format(300))\n",
    "    header = 'lr,tree,depth,subsample,colsample,auc_train,auc_test'\n",
    "    with open(tuning_path, 'w') as f:\n",
    "        f.write(header)\n",
    "        f.write('\\n')\n",
    "    for i in range(300):\n",
    "        lr = uniform(low=0.01, high=0.1)\n",
    "        tree = randint(low=100, high=1000)\n",
    "        depth = randint(low=3, high=6)\n",
    "        subsample = uniform(low=0.5, high=1)\n",
    "        colsample = uniform(low=0.5, high=1)\n",
    "        params = {'learning_rate': lr,\n",
    "                'n_estimators': tree,\n",
    "                'max_depth': depth,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample,\n",
    "                'objective': 'multi:softmax',\n",
    "                'random_state': 42}\n",
    "        xgb = XGBClassifier(**params)\n",
    "        xgb.fit(X_uni_train_over, y_train_over)\n",
    "\n",
    "        # train\n",
    "        train_preds = xgb.predict(X_uni_train_over)\n",
    "        train_f1score = f1_score(train_preds, y_train_over, average='weighted')\n",
    "        \n",
    "        # test\n",
    "        test_preds = xgb.predict(X_uni_test_over)\n",
    "        test_f1score = f1_score(test_preds, y_test_over, average='weighted')\n",
    "\n",
    "        line = ','.join(str(i) for i in [lr, tree, depth, subsample, colsample, \n",
    "                                            train_f1score, test_f1score])\n",
    "        # print(\"Line {} is written into result\".format(line))\n",
    "        with open(tuning_path, 'a') as f:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "        print(\"Hyperparameter round {} : {} is finished\".format(i, line))\n",
    "    print(\"~~~~Hyperparameter tuning done~~~~\")\n",
    "\n",
    "xgb_uni_tuning = pd.read_csv(tuning_path)\n",
    "xgb_uni_tuning.sort_values('auc_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8694276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/data.py:365: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    }
   ],
   "source": [
    "xgb_uni_params = {\n",
    "    'learning_rate': 0.091607,\n",
    "    'n_estimators': 843,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.871868,\n",
    "    'colsample_bytree': 0.827515,\n",
    "    'objective': 'multi:softmax',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "xgb_uni_over = XGBClassifier(**xgb_uni_params)\n",
    "xgb_uni_over.fit(X_uni_train_over, y_train_over)\n",
    "xgb_uni_preds_over = xgb_uni_over.predict(X_uni_test_over)\n",
    "print(accuracy_score(xgb_uni_preds_over,y_test_over))\n",
    "print(f1_score(xgb_uni_preds_over, y_test_over, average='weighted'))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test_over, xgb_uni_preds_over)\n",
    "confusion_matrix\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "# import joblib\n",
    "# joblib.dump(xgb_uni_over,r'/Users/ethanshen/Documents/UIUC/Fa23/CS410/GroupProject/unigram_xgboost_oversample.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c772dcb",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88ec71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_vectorizer = TfidfVectorizer(strip_accents=None,\n",
    "                             lowercase=False,\n",
    "                             preprocessor=None,\n",
    "                             ngram_range = (2,2))\n",
    "\n",
    "\n",
    "X_bi = bi_vectorizer.fit_transform(df[\"Comment\"].values.astype('U'))\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "X_bi_train, X_bi_test, y_train, y_test = train_test_split(X_bi, y, train_size=0.70, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e7f8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8507653061224489\n",
      "0.9193659545141281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,  257],\n",
       "       [   0,    0,   94],\n",
       "       [   0,    0, 2001]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_bi = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(X_bi_train, y_train)\n",
    "logreg_bi_preds = logreg_bi.predict(X_bi_test)\n",
    "logreg_bi_preds_proba = logreg_bi.predict_proba(X_bi_test)\n",
    "\n",
    "print(accuracy_score(logreg_bi_preds,y_test))\n",
    "print(f1_score(logreg_bi_preds, y_test, average='weighted'))\n",
    "cm = metrics.confusion_matrix(y_test, logreg_bi_preds)\n",
    "cm\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff132da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6b80b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos, Neu, Neg rates in train are 33.33% 33.33% 33.33%\n",
      "Pos, Neu, Neg rates in test are 85.08% 4.00% 10.93%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE(random_state = 42)\n",
    "X_bi_train_over, y_train_over = sm.fit_resample(X_bi_train, y_train)\n",
    "X_bi_test_over, y_test_over = X_bi_test.copy(), y_test.copy()\n",
    "print(\"Pos, Neu, Neg rates in train are {:.2%} {:.2%} {:.2%}\".format((y_train_over == 2).sum() / y_train_over.shape[0], (y_train_over == 1).sum() / y_train_over.shape[0], (y_train_over == 0).sum() / y_train_over.shape[0]))\n",
    "print(\"Pos, Neu, Neg rates in test are {:.2%} {:.2%} {:.2%}\".format((y_test_over == 2).sum() / y_test_over.shape[0], (y_test_over == 1).sum() / y_test_over.shape[0], (y_test_over == 0).sum() / y_test_over.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb2b0b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8507653061224489\n",
      "0.9193659545141281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,  257],\n",
       "       [   0,    0,   94],\n",
       "       [   0,    0, 2001]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_bi_over = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(X_bi_train_over, y_train_over)\n",
    "logreg_bi_preds_over = logreg_bi.predict(X_bi_test_over)\n",
    "\n",
    "print(accuracy_score(logreg_bi_preds_over, y_test_over))\n",
    "print(f1_score(logreg_bi_preds_over, y_test_over, average='weighted'))\n",
    "cm = metrics.confusion_matrix(y_test_over, logreg_bi_preds_over)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d98a4e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efecc4",
   "metadata": {},
   "source": [
    "# Binary Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbe2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment_V2'] = np.where(\n",
    "    df['Rating'].isin([1,2,3]), 0,\n",
    "    np.where(\n",
    "        df['Rating'].isin([4,5]), 1, np.nan\n",
    "    )\n",
    ")\n",
    "df['Sentiment_V2'].value_counts(True,dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_vectorizer = TfidfVectorizer(strip_accents=None,\n",
    "                             lowercase=False,\n",
    "                             preprocessor=None,\n",
    "                             ngram_range = (1,1))\n",
    "\n",
    "\n",
    "X_uni = uni_vectorizer.fit_transform(df[\"Comment_v2\"].values.astype('U'))\n",
    "y = df[\"Sentiment_V2\"]\n",
    "\n",
    "X_uni_train, X_uni_test, y_train, y_test = train_test_split(X_uni, y, train_size=0.70, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_uni = LogisticRegression(solver='lbfgs', random_state = 42).fit(X_uni_train, y_train)\n",
    "logreg_uni_preds = logreg_uni.predict(X_uni_test)\n",
    "print(accuracy_score(logreg_uni_preds,y_test))\n",
    "print(f1_score(logreg_uni_preds, y_test, average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_test, logreg_uni_preds)\n",
    "cm\n",
    "# cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_path = r'/Users/ethanshen/Documents/UIUC/Fa23/CS410/GroupProject/unigram_xgboost_posneg.csv'\n",
    "if os.path.exists(tuning_path):\n",
    "    pass\n",
    "else:\n",
    "    print(\"Now we're tuning hyperparameters for {} iterations ~~>=__=<~~\".format(300))\n",
    "    header = 'lr,tree,leaf,depth,subsample,colsample,auc_train,auc_test'\n",
    "    with open(tuning_path, 'w') as f:\n",
    "        f.write(header)\n",
    "        f.write('\\n')\n",
    "    for i in range(300):\n",
    "        lr = uniform(low=0.01, high=0.1)\n",
    "        tree = randint(low=100, high=1000)\n",
    "        leaf = randint(low=3, high=8)\n",
    "        depth = randint(low=3, high=6)\n",
    "        subsample = uniform(low=0.5, high=1)\n",
    "        colsample = uniform(low=0.5, high=1)\n",
    "        params = {'learning_rate': lr,\n",
    "                'n_estimators': tree,\n",
    "                'num_leaves': leaf,\n",
    "                'max_depth': depth,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample,\n",
    "                'objective': 'binary:logistic',\n",
    "                'random_state': 42}\n",
    "        xgb = XGBClassifier(**params)\n",
    "        xgb.fit(X_uni_train, y_train)\n",
    "\n",
    "        # train\n",
    "        train_preds = xgb.predict(X_uni_train)\n",
    "        train_f1score = f1_score(train_preds, y_train, average='weighted')\n",
    "        \n",
    "        # test\n",
    "        test_preds = xgb.predict(X_uni_test)\n",
    "        test_f1score = f1_score(test_preds, y_test, average='weighted')\n",
    "\n",
    "        line = ','.join(str(i) for i in [lr, tree, leaf, depth, subsample, colsample, \n",
    "                                            train_f1score, test_f1score])\n",
    "        # print(\"Line {} is written into result\".format(line))\n",
    "        with open(tuning_path, 'a') as f:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "        print(\"Hyperparameter round {} : {} is finished\".format(i, line))\n",
    "    print(\"~~~~Hyperparameter tuning done~~~~\")\n",
    "\n",
    "xgb_uni_tuning = pd.read_csv(tuning_path)\n",
    "xgb_uni_tuning.sort_values('auc_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_uni_tuning.sort_values('auc_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5271d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "833db68fc5bc536e2fd15ea96b5158b720dd421467038326c0e17ecc712ca437"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
